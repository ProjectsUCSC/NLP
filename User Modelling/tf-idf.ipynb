{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import enchant\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from collections import Counter\n",
    "from keras.models import model_from_json\n",
    "import math\n",
    "import signal\n",
    "import h5py\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#feature extraction - TFIDF and unigrams\n",
    "def vec(preprocessed_data_sample):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.\n",
    "#    no_features = 1000#500#806#150#800#600#350\n",
    "\n",
    "    #ngram_range=(1, 1)\n",
    "#    input=u'content', encoding=u'utf-8', decode_error=u'strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer=u'word', stop_words=None, token_pattern=u'(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<type 'numpy.int64'>, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False\n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, vocabulary = None, ngram_range=(1,1), strip_accents=None)#, max_features = no_features)#, ngram_range=(2,2))\n",
    "    #vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, ngram_range=(2,2), max_features = no_features)\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of\n",
    "    # strings.\n",
    "    train_data_features = vectorizer.fit_transform(preprocessed_data_sample)\n",
    "\n",
    "    # Numpy arrays are easy to work with, so convert the result to an\n",
    "    # array\n",
    "    train_data_features = train_data_features.toarray()\n",
    "    return [train_data_features, vectorizer, no_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import enchant\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from collections import Counter\n",
    "from keras.models import model_from_json\n",
    "import math\n",
    "import signal\n",
    "import h5py\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "import codecs\n",
    "import pylab as plot\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "def handler(signum, frame):\n",
    "    print 'Ctrl+Z pressed'\n",
    "    assert False\n",
    "signal.signal(signal.SIGTSTP, handler)\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "def readData(filename1, filename2):\n",
    "    cwd = os.getcwd()\n",
    "    req_attributes = ['tweet_id', 'topic', 'sentiment', 'tweet', 'user_id']#, 'followers_count', 'statuses_count', 'description', 'friends_count', 'location']\n",
    "    user_req_attributes = ['tweet_id', 'user_id']#, 'followers_count', 'statuses_count', 'description', 'friends_count', 'location']\n",
    "    tweet_req_attributes = ['tweet_id', 'topic', 'sentiment', 'tweet']\n",
    "    path = cwd + \"/data/\" + filename1;\n",
    "    tweet_df = pd.read_csv(path, sep='\\t');\n",
    "    \n",
    "    tweet_df = tweet_df.drop_duplicates(['tweet_id'])\n",
    "    tweet_df = tweet_df[tweet_req_attributes]\n",
    "\n",
    "    path = cwd + \"/data/\" + filename2;\n",
    "    user_df = pd.read_csv(path, sep='\\t')\n",
    "    user_df = user_df.dropna(subset=['user_id'])\n",
    "    user_df = user_df[user_req_attributes]\n",
    "    \n",
    "    data = tweet_df.merge(user_df, left_on=\"tweet_id\", right_on=\"tweet_id\", how=\"inner\")\n",
    "    data = data.drop_duplicates(['tweet_id'])\n",
    "    data = data.dropna(subset=['user_id', 'tweet'])\n",
    "    data = data.dropna(subset=['user_id'])\n",
    "    print len(user_df), len(tweet_df), len(data)\n",
    "    \n",
    "    print \"From user data\\n\", Counter(list(data[\"user_id\"])).most_common(50)\n",
    "    return data[req_attributes]\n",
    "\n",
    "\n",
    "def tokenize_and_stopwords(data_sample):\n",
    "\n",
    "    print type(data_sample)\n",
    "    print len(data_sample)\n",
    "    #Get all english stopwords\n",
    "    try:\n",
    "        words = open(\"common_words.txt\", \"r\").readlines()\n",
    "        for i in range(len(words)):\n",
    "            words[i] = words[i].strip()\n",
    "    except: \n",
    "        words = []\n",
    "    print \"words\", words\n",
    "    #abb_dict = pickle.load(open(\"abbreviations\", \"r\"))\n",
    "    stop = stopwords.words('english') + words #list(string.punctuation) + ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    #Use only characters from reviews\n",
    "    data_sample = data_sample.str.replace(\"[^a-zA-Z ]\", \" \")#, \" \")\n",
    "    data_sample = data_sample.str.lower()\n",
    "                \n",
    "    return [(\" \").join([i for i in sentence.split() if i not in stop]) for sentence in data_sample]\n",
    "\n",
    "def cleanhtml(tweet):\n",
    "      cleanr = re.compile('<.*?>')\n",
    "      cleantext = re.sub(cleanr, '', tweet)\n",
    "      return cleantext\n",
    "\n",
    "def cleanUrl(tweet):\n",
    "    tweet= re.sub(r\"http\\S+\", \"\",  tweet)\n",
    "    return tweet;\n",
    "\n",
    "def removeMention(tweet):\n",
    "    tweet = tweet.replace(\"rt@\",\"\").rstrip()\n",
    "    tweet = tweet.replace(\"rt \",\"\").rstrip()\n",
    "    tweet = tweet.replace(\"@\",\"\").rstrip()\n",
    "    return tweet;\n",
    "\n",
    "def stemmer(preprocessed_data_sample):\n",
    "    print \"stemming \"\n",
    "    #Create a new Porter stemmer.\n",
    "    stemmer = PorterStemmer()\n",
    "    #try:\n",
    "    for i in range(len(preprocessed_data_sample)):\n",
    "        #Stemming\n",
    "        try:\n",
    "            preprocessed_data_sample[i] = preprocessed_data_sample[i].replace(preprocessed_data_sample[i], \" \".join([stemmer.stem(str(word)) for word in preprocessed_data_sample[i].split()]))\n",
    "        except:\n",
    "        #No stemming\n",
    "            preprocessed_data_sample[i] = preprocessed_data_sample[i].replace(preprocessed_data_sample[i], \" \".join([str(word) for word in preprocessed_data_sample[i].split()]))\n",
    "    return preprocessed_data_sample\n",
    "\n",
    "    \n",
    "#feature extraction - TFIDF and unigrams\n",
    "def vectorize(preprocessed_data_sample):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.\n",
    "#    no_features = 1000#500#806#150#800#600#350\n",
    "\n",
    "    #ngram_range=(1, 1)\n",
    "#    input=u'content', encoding=u'utf-8', decode_error=u'strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer=u'word', stop_words=None, token_pattern=u'(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<type 'numpy.int64'>, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False\n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, vocabulary = None, ngram_range=(1,1), strip_accents=None)#, max_features = no_features)#, ngram_range=(2,2))\n",
    "    #vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, ngram_range=(2,2), max_features = no_features)\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of\n",
    "    # strings.\n",
    "    train_data_features = vectorizer.fit_transform(preprocessed_data_sample)\n",
    "\n",
    "    # Numpy arrays are easy to work with, so convert the result to an\n",
    "    # array\n",
    "    train_data_features = train_data_features.toarray()\n",
    "    return [train_data_features, vectorizer, no_features]\n",
    "\n",
    "    \n",
    "def preprocess(filename1, filename2):\n",
    "    #filename = \"Homework2_data.csv\"\n",
    "    df = readData(filename1, filename2)\n",
    "    print \"from joined data\\n\", Counter(list(df[\"user_id\"])).most_common(50)\n",
    "    indices = []\n",
    "#    df['tweet'] = df['tweet'].apply(cleanhtml).apply(cleanUrl).apply(removeMention).apply(removeTrailingHash);\n",
    "\n",
    "    df['tweet'] = df['tweet'].apply(cleanhtml).apply(cleanUrl)#.apply(removeTrailingHash);\n",
    "    df['tweet'] = tokenize_and_stopwords(df['tweet'])\n",
    "    data = DataFrame(df.groupby('topic')['tweet'].apply(list)).reset_index()\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data['tweet'][i] = \" \".join(data['tweet'][i])\n",
    "    \n",
    "    topics = list(data[\"topic\"])\n",
    "#    Watch out\n",
    "#.   topics = topics[0:10]\n",
    "#    Word topic mapping\n",
    "    try:\n",
    "        word_dict = pickle.load(open(\"word_dict\", \"r\"))\n",
    "    except:\n",
    "        tweets = \"\"\n",
    "        for index, i in data.iterrows():\n",
    "#        for i in data['tweet']:\n",
    "            if i['topic'] in topics:\n",
    "                tweets += str(i['tweet'])\n",
    "        \n",
    "        word_dict = {}\n",
    "        tweets = tweets.split()\n",
    "        for word in tweets:\n",
    "            word_dict[word] = []\n",
    "            for i in range(len(topics)):\n",
    "                if word in data[\"tweet\"][i]:\n",
    "                    word_dict[word].append(topics[i])        \n",
    "        pickle.dump(word_dict, open(\"word_dict\", \"wb\"))\n",
    "    \n",
    "    print \"the word 'election' is present in\", (word_dict['register'])\n",
    "    print len(word_dict)\n",
    "    return df, topics\n",
    "#    Word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = os.getcwd() + \"/data/\"\n",
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "filename1 = \"tweets.txt\"#twitter-2016dev-CE-output.txt_semeval_tweets.txt\"\n",
    "print filename1\n",
    "filename2 = \"users.txt\"#\"twitter-2016dev-CE-output.txt_semeval_userinfo.txt\"\n",
    "print filename2\n",
    "df, topics =preprocess(filename1, filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "all_text = df[\"tweet\"].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "df['tokenized_sents'] = df.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gensim.models.word2vec as wv\n",
    "model = wv.Word2Vec(df[\"tokenized_sents\"], size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.save(\"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec\")\n",
    "#model.similarity(\"this\", \"is\")\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getEmbedding(sentence):\n",
    "    list = np.array([])\n",
    "    for word in sentence:\n",
    "        if word in model.wv.vocab:\n",
    "             list = np.append(list, model.wv[word])\n",
    "    \n",
    "    #print list.size\n",
    "    if(list.size > 5000):\n",
    "        list = list[0:5000]\n",
    "    #print sentence\n",
    "    pad = np.zeros(5000 - list.size)\n",
    "    list = np.append(list, pad)\n",
    "    #print list.shape\n",
    "    return list\n",
    "    \n",
    "#getEmbedding(df['tokenized_sents'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print df.size\n",
    "df['embedding'] = df['tokenized_sents'].apply(getEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "genre = np.array(['tech', 'politics', 'music', 'sports'])\n",
    "tech = np.array(['@microsoft', 'nokia', 'amazon', 'amazon prime', 'amazon prime day', 'apple', 'apple watch', 'ipad', 'iphone', 'ipod', 'oracle', 'ibm', 'nintendo', 'moto g', 'google', 'google +', 'ps4', 'netflix'])                        \n",
    "\n",
    "politics = np.array(['angela merkel',  'bernie sanders', 'david cameron',' donald trump', 'hillary', 'joe biden', 'michelle obama', 'obama', 'rahul gandhi', 'tony blair'])\n",
    "\n",
    "music = np.array(['bee gees', 'beyonce', 'bob marley', 'chris brown', 'david bowie', 'katy perry',  'ed sheeran', 'foo fighters', 'janet jackson', 'lady gaga', 'michael jackson',  'ac/dc', 'the vamps', 'iron maiden', 'rolling stone', 'jay-z', 'snoop dogg', 'nirvana'])\n",
    "\n",
    "sports = np.array(['arsenal', 'barca', 'federer', 'floyd mayweather', 'hulk hogan', 'john cena', 'kris bryant', 'randy orton', 'real madrid', 'serena', 'messi', 'david beckham', 'rousey', 'super eagles', 'kane', 'red sox', 'white sox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shubhi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "all_topics = np.concatenate((tech, politics, music, sports))\n",
    "df_filter = df[df[\"topic\"].isin(all_topics)]\n",
    "topics_array = np.array(([tech, politics, music, sports]))\n",
    "df_filter['topic_id'] = df_filter['topic'].apply(getTopicId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shubhi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "def getTopicId(topic):\n",
    "    return all_topics.tolist().index(topic) \n",
    "df_filter['topic_id'] = df_filter['topic'].apply(getTopicId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#print df.columns\n",
    "import pandas as pd \n",
    "one_hot = pd.get_dummies(all_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shubhi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df_filter['vector'] = df_filter['embedding'] # + one_hot[df['topic']].T\n",
    "for index, row in df_filter.iterrows():\n",
    "    one_hot_encoding  =  one_hot[row['topic']]\n",
    "    row['vector'] = np.concatenate([row['vector'], one_hot_encoding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#list contains objects and you want to copy them as well, use generic copy.deepcopy():\n",
    "\n",
    "import copy\n",
    "#new_list = copy.deepcopy(old_list)\n",
    "X =copy.deepcopy( np.vstack(df_filter['embedding'][0:5000]))\n",
    "X_test = copy.deepcopy(np.vstack(df_filter['embedding'][5001:6357]))\n",
    "Y = copy.deepcopy(df_filter['sentiment'][0:5000])\n",
    "Y_test=copy.deepcopy(df_filter['sentiment'][5001:6357])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'0': 10125, '1': 9576, '-1': 2483, '2': 692, '-2': 200})\n",
      "Counter({'0': 2994, '1': 2808, '-1': 617, '2': 269, '-2': 69})\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "import collections\n",
    "counter=collections.Counter(df['sentiment'])\n",
    "print counter\n",
    "counter2=collections.Counter(df_filter['sentiment'])\n",
    "print counter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print Y\n",
    "print X.shape\n",
    "print Y.shape\n",
    "Y_train = Y\n",
    "Y_train[ Y_train < 0] = 0\n",
    "Y_train[ Y_train == 0] = 1\n",
    "Y_train[ Y_train > 0] = 2\n",
    "print Y_train, Y_test\n",
    "#np.reshape(X,(20000, 5000) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#np.reshape(Y_train, (1,5000))\n",
    "print X.shape\n",
    "print Y.shape\n",
    "clf = svm.SVC()\n",
    "clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y_pred_new = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y_pred = np.array(Y_pred)\n",
    "Y_pred[Y_pred < 0] = -1\n",
    "Y_pred[Y_pred == 0] = 0\n",
    "Y_pred[Y_pred > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y_test[np.where(Y_test < 0)] = -1\n",
    "Y_test[np.where(Y_test == 0)] = 0\n",
    "Y_test[np.where(Y_test > 0)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def groupClasses(Y) :\n",
    "    Y[Y < 0] = -1\n",
    "    Y[Y == 0] = 0\n",
    "    Y[Y > 0] = 1\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 5000})\n",
      "Counter({1: 5000})\n",
      "Counter({2: 1356})\n",
      "Counter({'1': 666, '0': 423, '-1': 153, '2': 89, '-2': 25})\n",
      "Counter({'0': 733, '1': 557, '-1': 65, '2': 1})\n"
     ]
    }
   ],
   "source": [
    "counter2=collections.Counter(Y)\n",
    "print counter2\n",
    "counter2=collections.Counter(pred_train)\n",
    "print counter2\n",
    "counter2=collections.Counter(Y_pred)\n",
    "print counter2\n",
    "counter2=collections.Counter(Y_test)\n",
    "print counter2\n",
    "counter2=collections.Counter(pred)\n",
    "print counter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic Regression\n"
     ]
    }
   ],
   "source": [
    "print \"logistic Regression\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logregr = LogisticRegression()\n",
    "logregr.fit(X, Y)\n",
    "pred = logregr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_train = logregr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6658\n",
      "0.411504424779\n"
     ]
    }
   ],
   "source": [
    "print accuracy_score(Y, pred_train)\n",
    "print accuracy_score(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y = groupClasses(Y)\n",
    "pred_train = groupClasses(pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word2topic = pickle.load(open(\"word2topic\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getEmbeddingWord2Topic(sentence):\n",
    "   \n",
    "    list = np.array([])\n",
    "    for word in sentence:\n",
    "        if word in keys:\n",
    "             list = np.append(list, word2topic[word])\n",
    "    \n",
    "    #print list.size\n",
    "    if(list.size > 5000):\n",
    "        list = list[0:5000]\n",
    "    #print sentence\n",
    "    pad = np.zeros(5000 - list.size)\n",
    "    list = np.append(list, pad)\n",
    "    #print list.shape\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "keys = word2topic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['embedding'] = df['tokenized_sents'].apply(getEmbeddingWord2Topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
