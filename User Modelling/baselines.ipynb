{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one time methods only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_topics():\n",
    "    genre = np.array(['tech', 'politics', 'music', 'sports'])\n",
    "    tech = np.array(['@microsoft', 'nokia', 'amazon', 'amazon prime', 'amazon prime day', 'apple', 'apple watch', 'ipad', 'iphone', 'ipod', 'oracle', 'ibm', 'nintendo', 'moto g', 'google', 'google +', 'ps4', 'netflix'])                        \n",
    "    politics = np.array(['angela merkel',  'bernie sanders', 'david cameron',' donald trump', 'hillary', 'joe biden', 'michelle obama', 'obama', 'rahul gandhi', 'tony blair'])\n",
    "    music = np.array(['bee gees', 'beyonce', 'bob marley', 'chris brown', 'david bowie', 'katy perry',  'ed sheeran', 'foo fighters', 'janet jackson', 'lady gaga', 'michael jackson',  'ac/dc', 'the vamps', 'iron maiden', 'rolling stone', 'jay-z', 'snoop dogg', 'nirvana'])\n",
    "    sports = np.array(['arsenal', 'barca', 'federer', 'floyd mayweather', 'hulk hogan', 'john cena', 'kris bryant', 'randy orton', 'real madrid', 'serena', 'messi', 'david beckham', 'rousey', 'super eagles', 'kane', 'red sox', 'white sox'])\n",
    "    all_topics = np.concatenate((tech, politics, music, sports))\n",
    "    return [all_topics, genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2topic_preprocess():\n",
    "    word2topic = pickle.load(open(\"word2topic\", \"r\"))\n",
    "    keys = word2topic.keys()\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTopicId(topic):\n",
    "    return all_topics.tolist().index(topic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec_preprocess(df):\n",
    "    from gensim.models import Word2Vec\n",
    "    import logging\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "        level=logging.INFO)\n",
    "    import gensim.models.word2vec as wv\n",
    "    model = wv.Word2Vec(df[\"tokenized_sents\"], size=100, window=5, min_count=5, workers=4)\n",
    "    model.save(\"word2vec\")\n",
    "    model = Word2Vec.load(\"word2vec\")\n",
    "    model.init_sims(replace=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #add one hot \n",
    "\n",
    "def one_hot_encoding():\n",
    "    one_hot = pd.get_dummies(all_topics)\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getEmbeddingWord2Topic(sentence) :  \n",
    "    num_of_words =30\n",
    "    size_of_vec = 100\n",
    "    embedding_size = num_of_words * size_of_vec\n",
    "    list = np.array([])\n",
    "    for word in sentence:\n",
    "        if word in keys:\n",
    "             list = np.append(list, word2topic[word])\n",
    "    if(list.size > embedding_size):\n",
    "        list = list[0:embedding_size]\n",
    "    pad = np.zeros(embedding_size - list.size)\n",
    "    list = np.append(list, pad)\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getEmbedding(sentence, ) :  \n",
    "    num_of_words =30\n",
    "    size_of_vec = 100\n",
    "    embedding_size = num_of_words * size_of_vec\n",
    "    list = np.array([])\n",
    "    for word in sentence:\n",
    "        if word in model.wv.vocab:\n",
    "             list = np.append(list, model.wv[word])\n",
    "    if(list.size > embedding_size):\n",
    "        list = list[0:embedding_size]\n",
    "    pad = np.zeros(embedding_size - list.size)\n",
    "    list = np.append(list, pad)\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X,Y):\n",
    "    print \"logistic Regression\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    logregr = LogisticRegression()\n",
    "    logregr.fit(X, Y)\n",
    "    pred = logregr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_features_to_df():\n",
    "    #filter and add topic id \n",
    "    df_filter = df[df[\"topic\"].isin(all_topics)]\n",
    "    topics_array = np.array(([tech, politics, music, sports]))\n",
    "    df_filter['topic_id'] = df_filter['topic'].apply(getTopicId)\n",
    "    return df_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_one_hot_encoding():\n",
    "    df_filter['vector'] = df_filter['embedding'] # + one_hot[df['topic']].T\n",
    "    for index, row in df_filter.iterrows():\n",
    "        one_hot_encoding  =  one_hot[row['topic']]\n",
    "        row['vector'] = np.concatenate([row['vector'], one_hot_encoding])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[X, y, df, d] = pickle.load(open(\"data_rnn\", \"r\"))\n",
    "[all_topics,genre] = get_all_topics()\n",
    "df['tokenized_sents'] = df.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##to get word2Vec embedding \n",
    "## preprocessing \n",
    "model =word2vec_preprocess(df)\n",
    "df['embedding'] = df['tokenized_sents'].apply(getEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2cddfd535115>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2topic_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_sents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetEmbeddingWord2Topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "keys = word2topic_preprocess()\n",
    "df['embedding'] = df['tokenized_sents'].apply(getEmbeddingWord2Topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_filter = add_features_to_df()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prepare test and train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "#new_list = copy.deepcopy(old_list)\n",
    "X =copy.deepcopy( np.vstack(df_filter['embedding'][0:5000]))\n",
    "X_test = copy.deepcopy(np.vstack(df_filter['embedding'][5001:6357]))\n",
    "Y = copy.deepcopy(df_filter['sentiment'][0:5000])\n",
    "Y_test=copy.deepcopy(df_filter['sentiment'][5001:6357])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
