{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import enchant\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from collections import Counter\n",
    "from keras.models import model_from_json\n",
    "import math\n",
    "import signal\n",
    "import h5py\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature extraction - TFIDF and unigrams\n",
    "def vec(preprocessed_data_sample):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.\n",
    "#    no_features = 1000#500#806#150#800#600#350\n",
    "\n",
    "    #ngram_range=(1, 1)\n",
    "#    input=u'content', encoding=u'utf-8', decode_error=u'strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer=u'word', stop_words=None, token_pattern=u'(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<type 'numpy.int64'>, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False\n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, vocabulary = None, ngram_range=(1,1), strip_accents=None)#, max_features = no_features)#, ngram_range=(2,2))\n",
    "    #vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, ngram_range=(2,2), max_features = no_features)\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of\n",
    "    # strings.\n",
    "    train_data_features = vectorizer.fit_transform(preprocessed_data_sample)\n",
    "\n",
    "    # Numpy arrays are easy to work with, so convert the result to an\n",
    "    # array\n",
    "    train_data_features = train_data_features.toarray()\n",
    "    return [train_data_features, vectorizer, no_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import enchant\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from collections import Counter\n",
    "from keras.models import model_from_json\n",
    "import math\n",
    "import signal\n",
    "import h5py\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "import codecs\n",
    "import pylab as plot\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "def handler(signum, frame):\n",
    "    print 'Ctrl+Z pressed'\n",
    "    assert False\n",
    "signal.signal(signal.SIGTSTP, handler)\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "def readData(filename1, filename2):\n",
    "    cwd = os.getcwd()\n",
    "    req_attributes = ['tweet_id', 'topic', 'sentiment', 'tweet', 'user_id']#, 'followers_count', 'statuses_count', 'description', 'friends_count', 'location']\n",
    "    user_req_attributes = ['tweet_id', 'user_id']#, 'followers_count', 'statuses_count', 'description', 'friends_count', 'location']\n",
    "    tweet_req_attributes = ['tweet_id', 'topic', 'sentiment', 'tweet']\n",
    "    path = cwd + \"/data/\" + filename1;\n",
    "    tweet_df = pd.read_csv(path, sep='\\t');\n",
    "    \n",
    "    tweet_df = tweet_df.drop_duplicates(['tweet_id'])\n",
    "    tweet_df = tweet_df[tweet_req_attributes]\n",
    "\n",
    "    path = cwd + \"/data/\" + filename2;\n",
    "    user_df = pd.read_csv(path, sep='\\t')\n",
    "    user_df = user_df.dropna(subset=['user_id'])\n",
    "    user_df = user_df[user_req_attributes]\n",
    "    \n",
    "    data = tweet_df.merge(user_df, left_on=\"tweet_id\", right_on=\"tweet_id\", how=\"inner\")\n",
    "    data = data.drop_duplicates(['tweet_id'])\n",
    "    data = data.dropna(subset=['user_id', 'tweet'])\n",
    "    data = data.dropna(subset=['user_id'])\n",
    "    print len(user_df), len(tweet_df), len(data)\n",
    "    \n",
    "    print \"From user data\\n\", Counter(list(data[\"user_id\"])).most_common(50)\n",
    "    return data[req_attributes]\n",
    "\n",
    "\n",
    "def tokenize_and_stopwords(data_sample):\n",
    "\n",
    "    print type(data_sample)\n",
    "    print len(data_sample)\n",
    "    #Get all english stopwords\n",
    "    try:\n",
    "        words = open(\"common_words.txt\", \"r\").readlines()\n",
    "        for i in range(len(words)):\n",
    "            words[i] = words[i].strip()\n",
    "    except: \n",
    "        words = []\n",
    "    print \"words\", words\n",
    "    abb_dict = pickle.load(open(\"abbreviations\", \"r\"))\n",
    "    stop = stopwords.words('english') + words #list(string.punctuation) + ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    #Use only characters from reviews\n",
    "    data_sample = data_sample.str.replace(\"[^a-zA-Z ]\", \" \")#, \" \")\n",
    "    data_sample = data_sample.str.lower()\n",
    "                \n",
    "    return [(\" \").join([i for i in sentence.split() if i not in stop]) for sentence in data_sample]\n",
    "\n",
    "def cleanhtml(tweet):\n",
    "      cleanr = re.compile('<.*?>')\n",
    "      cleantext = re.sub(cleanr, '', tweet)\n",
    "      return cleantext\n",
    "\n",
    "def cleanUrl(tweet):\n",
    "    tweet= re.sub(r\"http\\S+\", \"\",  tweet)\n",
    "    return tweet;\n",
    "\n",
    "def removeMention(tweet):\n",
    "    tweet = tweet.replace(\"rt@\",\"\").rstrip()\n",
    "    tweet = tweet.replace(\"rt \",\"\").rstrip()\n",
    "    tweet = tweet.replace(\"@\",\"\").rstrip()\n",
    "    return tweet;\n",
    "\n",
    "def stemmer(preprocessed_data_sample):\n",
    "    print \"stemming \"\n",
    "    #Create a new Porter stemmer.\n",
    "    stemmer = PorterStemmer()\n",
    "    #try:\n",
    "    for i in range(len(preprocessed_data_sample)):\n",
    "        #Stemming\n",
    "        try:\n",
    "            preprocessed_data_sample[i] = preprocessed_data_sample[i].replace(preprocessed_data_sample[i], \" \".join([stemmer.stem(str(word)) for word in preprocessed_data_sample[i].split()]))\n",
    "        except:\n",
    "        #No stemming\n",
    "            preprocessed_data_sample[i] = preprocessed_data_sample[i].replace(preprocessed_data_sample[i], \" \".join([str(word) for word in preprocessed_data_sample[i].split()]))\n",
    "    return preprocessed_data_sample\n",
    "\n",
    "    \n",
    "#feature extraction - TFIDF and unigrams\n",
    "def vectorize(preprocessed_data_sample):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "    # bag of words tool.\n",
    "#    no_features = 1000#500#806#150#800#600#350\n",
    "\n",
    "    #ngram_range=(1, 1)\n",
    "#    input=u'content', encoding=u'utf-8', decode_error=u'strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer=u'word', stop_words=None, token_pattern=u'(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<type 'numpy.int64'>, norm=u'l2', use_idf=True, smooth_idf=True, sublinear_tf=False\n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, vocabulary = None, ngram_range=(1,1), strip_accents=None)#, max_features = no_features)#, ngram_range=(2,2))\n",
    "    #vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, ngram_range=(2,2), max_features = no_features)\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of\n",
    "    # strings.\n",
    "    train_data_features = vectorizer.fit_transform(preprocessed_data_sample)\n",
    "\n",
    "    # Numpy arrays are easy to work with, so convert the result to an\n",
    "    # array\n",
    "    train_data_features = train_data_features.toarray()\n",
    "    return [train_data_features, vectorizer, no_features]\n",
    "\n",
    "    \n",
    "def preprocess(filename1, filename2):\n",
    "    #filename = \"Homework2_data.csv\"\n",
    "    df = readData(filename1, filename2)\n",
    "    print \"from joined data\\n\", Counter(list(df[\"user_id\"])).most_common(50)\n",
    "    indices = []\n",
    "#    df['tweet'] = df['tweet'].apply(cleanhtml).apply(cleanUrl).apply(removeMention).apply(removeTrailingHash);\n",
    "\n",
    "    df['tweet'] = df['tweet'].apply(cleanhtml).apply(cleanUrl)#.apply(removeTrailingHash);\n",
    "    df['tweet'] = tokenize_and_stopwords(df['tweet'])\n",
    "    data = DataFrame(df.groupby('topic')['tweet'].apply(list)).reset_index()\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data['tweet'][i] = \" \".join(data['tweet'][i])\n",
    "    \n",
    "    topics = list(data[\"topic\"])\n",
    "#    Watch out\n",
    "    topics = topics[0:10]\n",
    "#    Word topic mapping\n",
    "    try:\n",
    "        word_dict = pickle.load(open(\"word_dict\", \"r\"))\n",
    "    except:\n",
    "        tweets = \"\"\n",
    "        for index, i in data.iterrows():\n",
    "#        for i in data['tweet']:\n",
    "            if i['topic'] in topics:\n",
    "                tweets += str(i['tweet'])\n",
    "        \n",
    "        word_dict = {}\n",
    "        tweets = tweets.split()\n",
    "        for word in tweets:\n",
    "            word_dict[word] = []\n",
    "            for i in range(len(topics)):\n",
    "                if word in data[\"tweet\"][i]:\n",
    "                    word_dict[word].append(topics[i])        \n",
    "        pickle.dump(word_dict, open(\"word_dict\", \"wb\"))\n",
    "    \n",
    "    print \"the word 'election' is present in\", (word_dict['register'])\n",
    "    print len(word_dict)\n",
    "#    Word model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shubhi/Public/NLP/User Modelling/data/\n"
     ]
    }
   ],
   "source": [
    "data = os.getcwd() + \"/data/\"\n",
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets.txt\n",
      "users.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shubhi/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2881: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159454 27973 23076\n",
      "From user data\n",
      "[('1240599805', 113), ('2887442487', 19), ('35907871', 18), ('2849708643', 16), ('178025398', 16), ('27035153', 14), ('291854750', 14), ('3002723962', 13), ('58825119', 12), ('528829450', 11), ('470503504', 11), ('548328925', 11), ('1249406600', 10), ('1612232178', 10), ('1732978867', 10), ('1283582990', 10), ('209723233', 8), ('1728778862', 8), ('365721480', 8), ('202896310', 8), ('3300472971', 7), ('381985881', 7), ('127666940', 7), ('2547936181', 7), ('310324954', 7), ('176075043', 7), ('317183842', 7), ('208244623', 6), ('58559901', 6), ('1895625649', 6), ('303077468', 6), ('44260055', 6), ('1217544986', 6), ('948697872', 6), ('3160618790', 6), ('53120768', 6), ('2652385123', 6), ('227817591', 5), ('1734934483', 5), ('2359028480', 5), ('3964749814', 5), ('138948791', 5), ('1351453770', 5), ('57411027', 5), ('898564909', 5), ('2233733934', 5), ('1220451187', 5), ('2437401410', 5), ('729683233', 5), ('3233810276', 5)]\n",
      "from joined data\n",
      "[('1240599805', 113), ('2887442487', 19), ('35907871', 18), ('2849708643', 16), ('178025398', 16), ('27035153', 14), ('291854750', 14), ('3002723962', 13), ('58825119', 12), ('528829450', 11), ('470503504', 11), ('548328925', 11), ('1249406600', 10), ('1612232178', 10), ('1732978867', 10), ('1283582990', 10), ('209723233', 8), ('1728778862', 8), ('365721480', 8), ('202896310', 8), ('3300472971', 7), ('381985881', 7), ('127666940', 7), ('2547936181', 7), ('310324954', 7), ('176075043', 7), ('317183842', 7), ('208244623', 6), ('58559901', 6), ('1895625649', 6), ('303077468', 6), ('44260055', 6), ('1217544986', 6), ('948697872', 6), ('3160618790', 6), ('53120768', 6), ('2652385123', 6), ('227817591', 5), ('1734934483', 5), ('2359028480', 5), ('3964749814', 5), ('138948791', 5), ('1351453770', 5), ('57411027', 5), ('898564909', 5), ('2233733934', 5), ('1220451187', 5), ('2437401410', 5), ('729683233', 5), ('3233810276', 5)]\n",
      "<class 'pandas.core.series.Series'>\n",
      "23076\n",
      "words []\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'abbreviations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c5667a924e79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"users.txt\"\u001b[0m\u001b[0;31m#\"twitter-2016dev-CE-output.txt_semeval_userinfo.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mfilename2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-64212781ade4>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(filename1, filename2)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanUrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.apply(removeTrailingHash);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_and_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'topic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-64212781ade4>\u001b[0m in \u001b[0;36mtokenize_and_stopwords\u001b[0;34m(data_sample)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"words\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mabb_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"abbreviations\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;31m#list(string.punctuation) + ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m#Use only characters from reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'abbreviations'"
     ]
    }
   ],
   "source": [
    "filename1 = \"tweets.txt\"#twitter-2016dev-CE-output.txt_semeval_tweets.txt\"\n",
    "print filename1\n",
    "filename2 = \"users.txt\"#\"twitter-2016dev-CE-output.txt_semeval_userinfo.txt\"\n",
    "print filename2\n",
    "preprocess(filename1, filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
